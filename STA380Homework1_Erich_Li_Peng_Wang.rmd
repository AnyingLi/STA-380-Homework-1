---
title: 'STA 380 Homework 1'
author: "Nicole Erich, Anying Li, Daniel Peng & Rachel Wang"
date: "August 5, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability practice

## Part A.

Based on law of total probability:
P(Y) = P(Y|RC) * P(RC) + P(Y|TC) * P(TC)

Here:
P(Y) = 0.65
P(Y|RC) = 0.5
P(RC) = 0.3
P(TC) = 0.7

Therefore:
P(Y|TC) = 0.7142857

So about 71.43% of people who are truthful clickers answered yes.

## Part B.

Based on Bayes' Rule:
P(A|B) = [P(A) * P(B|A)]/ P(B)

Here:

Event A is someone has the disease; event B is someone's test result is positive. We want to know P(A|B).

P(A) = 0.000025
P(B|A) = 0.993
We can calculate P(B) based on law of total probability:
P(B) = P(B|A) * P(A) + P(B|not A) * P(not A) 
     = 0.993 * 0.000025 + (1 - 0.9999) * (1 - 0.000025)
     = 0.0001248225

So P(A|B) = (0.000025 * 0.993) / 0.0001248225
          = 0.1988824

# Q1 Exploratory analysis: green buildings

```{r}
all.buildings = read.csv('https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv')
green.buildings = subset(all.buildings,all.buildings$Energystar == 1 | all.buildings$LEED == 1)
not.green = all.buildings[!(all.buildings$CS_PropertyID %in% green.buildings$CS_PropertyID),]
```

To estimate the economic impact of a green certificate, we had to calculate the expected extra profit brought in with it. To do so, We needed to find out the additional cost and revenue associated with a green building. The extra cost for this property is the $5 Million premium ($100M x 5%). The extra revenue per year would be additional rent/sqft-year x size of the building (250,000 sqft). In different clusters, we might value a green certificate differently. Therefore, we could not simply find the median rent for regular buildings and green buildings and subtract one from the other. Naturally, we were going to find the difference between two types of buildings by clusters. In this case, we treated all the regular buidlings in a certain cluster as control group so we could see the effect of a certificate.
```{r}
#calculate the average rent within each cluster
notgreen.mean.rent = aggregate(not.green$Rent, list(not.green$cluster), mean)
green.mean.rent = aggregate(green.buildings$Rent, list(green.buildings$cluster), mean)

#remove the clusters without any green buildings
notgreen.mean.rent = notgreen.mean.rent[which(notgreen.mean.rent[,1] %in% green.buildings$cluster),]

#remove the clusters withou any regualr buildings
green.mean.rent = green.mean.rent[which(green.mean.rent[,1] %in% notgreen.mean.rent[,1]),]

rent.diff = green.mean.rent - notgreen.mean.rent
boxplot(rent.diff$x, ylab = '$', xlab = 'green building rent premium')
rent = median(rent.diff$x)
c('expected additional rent:', rent)
```
From the boxplot above, we can see the majority of the rent premium is concentrated around $2, with many outliers. So It's better to use median of $2.66 for estimating the new building.

We used the same approach to calculate the exptected occupancy rate change for the new property.
```{r}
#calculate the average leasing rate within each cluster
notgreen.mean.lr = aggregate(not.green$leasing_rate, list(not.green$cluster), mean)
green.mean.lr = aggregate(green.buildings$leasing_rate, list(green.buildings$cluster), mean)

#remove the clusters without any green buildings
notgreen.mean.lr = notgreen.mean.lr[which(notgreen.mean.lr[,1] %in% green.buildings$cluster),]

#remove the clusters without any green buildings
green.mean.lr = green.mean.lr[which(green.mean.lr[,1] %in% notgreen.mean.lr[,1]),]

lr.diff = green.mean.lr - notgreen.mean.lr
boxplot(lr.diff$x, ylab = 'percentage', xlab = 'leasing rate difference between green vs non-green building')

lr = median(lr.diff$x)
c('expected additional occupancy:', lr)
```
Interestingly, the distribution of the occupancy difference is very concentrated around 10. It seems people prefer green buildings.

In order to find a good estimate for the occupancy rate for the new building, we plotted the distribution of all existing green buildings. The median of 92.93 looked like a good choice based on the boxplot.
```{r}
boxplot(green.mean.lr[,2], ylab = 'percentage', xlab = 'green building leasing rate')
```

In conclusion, our finding is very similar to the original analysis even though with different approach: rent premium = $2.66, occupancy rante = 92.93%.
```{r}
c('Number of Years to Break Even: ',5000000/(2.66 * 250000 * .9293))
c('Additional Annual Revenue for Green Certification:', (2.66 * 250000 * .9293))
```



# Q2 Bootstrapping

et up and create the function for calculating percent returns.
```{r}
rm(list=ls())
library(mosaic)
library(fImport)
library(foreach)
my_favorite_seed = 1234567
set.seed(my_favorite_seed)
YahooPricesToReturns = function(series) {
 	mycols = grep('Adj.Close', colnames(series))
 	closingprice = series[,mycols]
 	N = nrow(closingprice)
 	percentreturn = as.data.frame(closingprice[2:N,]) / as.data.frame(closingprice[1:(N-1),]) - 1
 	mynames = strsplit(colnames(percentreturn), '.', fixed=TRUE)
 	mynames = lapply(mynames, function(x) return(paste0(x[1], ".PctReturn")))
 	colnames(percentreturn) = mynames
 	as.matrix(na.omit(percentreturn))
 }
```

Import the 5 asset classes and calculate their repsective standard deviations.

```{r}
mystocks = c("SPY",'TLT','LQD','EEM','VNQ')
myprices = yahooSeries(mystocks, from='2011-01-01', to='2016-08-03')
myreturns = YahooPricesToReturns(myprices)
sigma_SPY = sd(myreturns[,1])
sigma_SPY
sigma_TLT = sd(myreturns[,2])
sigma_TLT
sigma_LQD = sd(myreturns[,3])
sigma_LQD
sigma_EEM = sd(myreturns[,4])
sigma_EEM
sigma_VNQ = sd(myreturns[,5])
sigma_VNQ
```

The standard deviations show that Emerging-market equities (EEM) and Real estate (VNQ) are the most volatile asset classes. We construct an aggressive portfolio with a  50/50 split between EEM and VNQ, a safe portfolio with the other three asset classes, and an even split by distributing 20% of money in each of the five ETFs.

```{r}
weights_safe = c(0.3, 0.3, 0.4, 0.0, 0.0)
weights_even = c(0.2, 0.2, 0.2, 0.2, 0.2)
weights_aggressive = c(0.0, 0.0, 0.0, 0.5, 0.5)
```

Use bootstrap resampling to estimate 4-week VaR of each of the three portfolios at the 5% level.

```{r}
n_days = 20
wealth_tracker_safe = rep(0, 5000)
wealth_tracker_even = rep(0, 5000)
wealth_tracker_aggressive = rep(0, 5000)
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
 	totalwealth = 100000
 	holdings_safe = weights_safe * totalwealth
 	holdings_even = weights_even * totalwealth
 	holdings_aggressive = weights_aggressive * totalwealth
 	for(today in 1:n_days) {
 		return.today = resample(myreturns, 1, orig.ids=FALSE)
 		holdings_safe = holdings_safe + holdings_safe*return.today
 		holdings_even = holdings_even + holdings_even*return.today
   	holdings_aggressive = holdings_aggressive + holdings_aggressive*return.today
		totalwealth_safe = sum(holdings_safe)
		totalwealth_even = sum(holdings_even)
		totalwealth_aggressive = sum(holdings_aggressive)
 		holdings_safe = weights_safe * totalwealth_safe
 		holdings_even = weights_even * totalwealth
   	holdings_aggressive = weights_aggressive * totalwealth_aggressive
 	}
 	wealth_tracker_safe[i]=totalwealth_safe
 	wealth_tracker_even[i]=totalwealth_even
 	wealth_tracker_aggressive[i]=totalwealth_aggressive
}
```

```{r}
var_safe = quantile(wealth_tracker_safe, 0.05) - 100000
var_safe
var_even = quantile(wealth_tracker_even, 0.05) - 100000
var_even
var_aggressive = quantile(wealth_tracker_aggressive, 0.05) - 100000
var_aggressive
```

The VaR analysis tells us that the theoretically safer portfolio is in fact more volatile than the even split. One reasonable explanation is that the risk of a portfolio is decided by its overall diversity rather than the standalone volatility of its components. Therefore, event split (best diversity) outperforms the "safer" portfolio comprised by the "safer" asset classes.

# Q3 Market segmentation

Step 1. In terms of data processing, first we get the frequency of each catogory by dividing each data by corresponding row sum, so we can get the relative weight of each category in each twitter. Then we try to limit the noise from categories such as "chatter","spam","adult" and "uncategorized" because these are not really user twitter with helpful information. We decide to get rid of the records if the sum of the  frequency of the four noise categories is bigger than 0.5. In the end, we scale and centralize the data to get the data prepared for clustering.

```{r }
library(cluster)
library(fpc)
library(flexclust)
library(foreach)
library(ggplot2)

social_marketing <- read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv", row.names=1)

# Center/scale the data
X_freq = social_marketing/rowSums(social_marketing)
dim(X_freq)
any(is.na(X_freq))

#take the record out if frequency of uninformative category is over 0.5
X_freq['sum'] = X_freq$chatter +X_freq$spam +X_freq$adult +X_freq$uncategorized
X_freq = subset(X_freq, X_freq$sum <= 0.5)
dim(X_freq)
X_freq <- X_freq[,-37]

social_marketing_scaled  = scale(X_freq, center=TRUE, scale=TRUE)

```

Step 2. We decide to begin with a k-mean clustering to see whether we can find something interesting. To use k-mean clustering, finding the appropriate k is our first task. To get the optimal k, we tried two approaches. First approach is to calculate the within group sum of square(wss). It's obvious that when k increases, wss will keep decreasing. But after plotting the relationship between k and wss, we can see that with 6 clusters the wss has already decreased significantly. As a second approach, we also calculated CH index that we discussed at class to decide the optimal k. For the seed we set, we get a optimal k equal to 12. But we should also notice that after 6 clusters, CH index does not improve significantly as k keeps increasing. Meanwhile, too many clusters can make it hard to intepret the meaning behind the clusering. Therefore, we decide to use k = 6 for the clustering.

```{r}
#decide optimal k based on within group sum of square
k.max <- 15  # Maximal number of clusters
data <- social_marketing_scaled

# Compute and plot wss for k = 2 to k = 15
set.seed(1234567)
wss <- sapply(1:k.max, 
        function(k){kmeans(data, k, nstart=50 )$tot.withinss})
plot(1:k.max, wss,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
abline(v = 6, lty =2)
#6 clusters are suggested

#decide optimal k based on within CH index/Average silhouette method
sil <- rep(0, k.max)

# Compute the average silhouette width for k = 2 to k = 15
for(i in 2:k.max){
  set.seed(1234567)
  km.res <- kmeans(data, centers = i, nstart = 50)
  ss <- silhouette(km.res$cluster, dist(data))
  sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)
abline(v = 6, lty =2)
# 12 clusters are suggested

```

Step3: After deciding the optimal k, we tried to build the clustering with both kmean and kmean++ for initialization, and it turns out that the error from these two methods are very close. It might be related with the seed we choose, and as we set nstart = 50 for kmean, it also helps imporve the accuracy of kmean clustering. 

After looking through the centers of the 6 clusters, we find:

second cluster has high positive weight on sports_fandom,food,family,religion,parenting and school,so this group may include married people who pay more attention to their family and parenting related topic.

third cluster has high positive weight on cooking,beauty and fashion,so this group should consist of younger woman and younger housewives who cook a lot and pay a lot of attention to beauty and fashion.

fourth cluster has high positive weight on politics,news,travel,computer and automotive,so this group might be younger man who are interested in politics, read lots of news online, loves computer and automotive and travels a lot.

fifth cluster has high positive weight on health_nutrition, outdoors and personal_fitness, so this group of people really care about nutrition and fitness, and they have lots of outdoor activity to keep fit.

sixth cluster has high positive weight on online_gaming,college_uni and sports_playing,this group is very likely to be college student whose main entertainment is online_gaming and sports.

first cluster's highest positive weight is on chatter, and nothing else really stands out. So we think this cluster may be just everything that is left out and is hard to get into any of the other market segment.

```{r}
# fit cluster model with 6 centers
set.seed(1234567)
cluster_6 <- kmeans(data, centers=6, nstart=50)
names(cluster_6)

cluster_6$centers

head(sort(cluster_6$centers[1,], decreasing=TRUE), 10)
head(sort(cluster_6$centers[2,], decreasing=TRUE), 10)
head(sort(cluster_6$centers[3,], decreasing=TRUE), 10)

plotcluster(data, cluster_6$cluster)
summary(cluster_6)
cluster_6$tot.withinss 

# use kmean++ for clustering initialization
set.seed(123)
cluster_kmeansPP = cclust(data, k=6, control=list(initcent="kmeanspp"))

parameters(cluster_kmeansPP)
cluster_kmeansPP@clusinfo

print(apply(parameters(cluster_kmeansPP),1,function(x) colnames(data)[order(x, decreasing=TRUE)[1:10]]))

# Roll our own function
centers = parameters(cluster_kmeansPP)
kpp_residualss = foreach(i=1:nrow(data), .combine='c') %do% {
	x = data[i,]
	a = cluster_kmeansPP@cluster[i]
	m = centers[a,]
	sum((x-m)^2)
}
sum(kpp_residualss)

```

step4: We also tried Principal Component Analysis on the dataset. However, we find that the variance of the dataset cannot be simply explained by the top2 or top3 factors. In fact, top10 pc all have a pretty strong explanation power for the dataset variance. And after we print out the top words associated with first and second PC, it's harder to interpret the "market segment" compared with using just the clustering. Therefore we just use PCA as a supporting evidence to what we find in kmean clustering.

```{r}
# PCA
pc = prcomp(X_freq, scale=TRUE)

set.seed(1234567)
loadings = pc$rotation
scores = pc$x

summary(pc)
plot(pc)
biplot(pc)
qplot(scores[,1], scores[,2], xlab='Component 1', ylab='Component 2')

# The top words associated with each component
o1 = order(loadings[,1])
colnames(X_freq)[head(o1,25)]
colnames(X_freq)[tail(o1,25)]

o2 = order(loadings[,2])
colnames(X_freq)[head(o2,25)]
colnames(X_freq)[tail(o2,25)]

```
seems pure clustering is just better